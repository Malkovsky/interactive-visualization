{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод сопряженных градиентов\n",
    "Этот метод также можно отнести к классу градиентных методов с моментом инерции, по крайней мере для квадратичных функций. Метод сопряженных градиентов является наиболее эффективным для минимизации квадратичных функций среди рассмотренных.\n",
    "\n",
    "<b>Определение</b>. Пусть $A$ -- симметричная матрица. Вектора $u, v$ называются <i>$A$-ортогональными</i> или <i>сопряженными</i>, если\n",
    "$$\n",
    "u^TAv=0.\n",
    "$$\n",
    "Как и ранее рассмотрим задачу минимизации\n",
    "$$\n",
    "f(x)=\\frac{1}{2}x^TAx-b^Tx+c,\n",
    "$$\n",
    "где $A$ - симметричная положительно определенная матрица, таким образом $\\nabla f(x)=Ax-b$, а значит нахождение точки минимума $f$ равносильно решению системы $Ax=b$. Обозначим за $x^*$ единственную точку минимума $f$.\n",
    "\n",
    "Предположим, что нам известны $n$ попарно сопряженных направлений $d_0, \\ldots, d_{n-1}$ относительно матрицы $A$. Выберем произвольную точку $x_0$ и сделаем по очереди $n$ шагов градиентного спуска по каждому из направлений, выбирая размер шага как минимум по направлению\n",
    "$$\n",
    "x_{k+1}=x_k-\\alpha_kd_k\n",
    "$$\n",
    "Получаем $\\alpha_k$ из уравнения $\\frac{d}{d\\alpha}f(x_k-\\alpha d_k)=0$:\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "0&=\\frac{d}{d\\alpha}f(x_k-\\alpha d_k)\\\\\n",
    "&=-d_k^T(A(x_k-\\alpha d_k)-b)\\\\\n",
    "&=\\alpha d^T_kAd^T_k-d_k^T(Ax_k-b)\n",
    "\\end{array}\n",
    "$$\n",
    "$$\n",
    "\\alpha_k=\\frac{d_k^T(Ax_k-b)}{d^T_kAd_k}.\n",
    "$$\n",
    "Теперь предположим, что $(d_0, \\ldots, d_{n-1})$ - базис в $\\mathbb{R}^n$, тогда\n",
    "$$\n",
    "x_0-x^*=\\sum_{i=0}^{n-1}\\delta_id_i\n",
    "$$\n",
    "Умножая это равенство на $d_k^TA$ получаем\n",
    "$$\n",
    "d_k^TA(x_0-x^*)=\\sum_{i=0}^{n-1}\\delta_id_k^TAd_i=\\delta_kd_k^TAd_k\n",
    "$$\n",
    "и получаем следующие равенства для $\\delta$\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\delta_k&=\\frac{d^T_kA(x_0-x^*)}{d_k^TAd_k}\\\\\n",
    "&=\\frac{d^T_kA(x_0-x^*-\\sum_{i=0}^{k-1}\\alpha_id_i)}{d_k^TAd_k}=\\frac{d^T_kA(x_k-x^*)}{d_k^TAd_k}=\\alpha_k\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом получаем\n",
    "$$\n",
    "x_k-x^*=x_0-\\sum_{i=0}^{k-1}\\alpha_id_i-x^*=\\sum_{i=0}^{n-1}\\alpha_id_i-\\sum_{i=0}^{k-1}\\alpha_id_i=\\sum_{i=k}^{n-1}\\alpha_id_i,\n",
    "$$\n",
    "что гарантирует сходимость этой процедуры за $n$ шагов. Более того,\n",
    "$$\n",
    "||x_k-x^*||^2_A=(x_k-x^*)^TA(x_k-x^*)=\\sum_{i=k}^{n-1}\\sum_{j=k}^{n-1}\\alpha_i\\alpha_jd_i^TAd_j=\\sum_{i=k}^{n-1}\\alpha_i^2d_i^TAd_i\n",
    "$$\n",
    "Пусть $x\\in x_0+\\langle d_0, \\ldots, d_{k-1}\\rangle$, т. е. $x=x_0+\\sum_{i=0}^{k-1}\\beta_id_i$, тогда\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "||x-x^*||^2_A&=\\left(\\sum_{i=0}^{k-1}\\beta_id_i+\\sum_{i=0}^{n-1}\\alpha_id_i\\right)^TA\\left(\\sum_{i=0}^{k-1}\\beta_id_i+\\sum_{i=0}^{n-1}\\alpha_id_i\\right)\\\\\n",
    "&=\\sum_{i=0}^{k-1}(\\beta_i+\\alpha_i)^2d_i^TAd_i+\\sum_{i=k}^{n-1}\\alpha_i^2d_i^TAd_i\\geq ||x_k-x^*||^2_A\n",
    "\\end{array}\n",
    "$$\n",
    "из чего следует важное свойство метода сопряженных направлений:\n",
    "$$\n",
    "x_k=argmin_{x\\in x_0+\\langle d_0, \\ldots, d_{k-1}\\rangle}||x-x^*||_A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остается вопрос: как найти $d_0, \\ldots, d_{n-1}$? Обратим внимание на то, что сопряженность является ортогональностью для скалярного произведения $\\langle x, y\\rangle_A=\\langle Ax, y\\rangle=\\langle x, Ay\\rangle=x^TAy$. Из этого свойства сразу вытекает возможность построения $d$ с помощью ортогонализации Грама-Шмидта: пусть $v_1, \\ldots, v_n\\in \\mathbb{R}^n$ - базис $\\mathbb{R}^n$, тогда для вектора $d_i$, определяемые как\n",
    "$$\n",
    "d_k=v_k-\\sum_{i=1}^{k-1}\\frac{d_i^TAv_k}{d_i^TAd_i}d_i\\tag{1}\n",
    "$$ \n",
    "являются попарно сопряженным базисом, т. е. как раз такими векторами, которые мы использовали. Подитоживая вышеописанные рассуждения можно получить следующий алгоритм:\n",
    "* Выбрать $n$ линейно независимых векторов $v_0, \\ldots, v_{n-1}$.\n",
    "* Построить $n$ сопряженных относительно матрицы $A$ направлений $d_0, \\ldots, d_{n-1}$ по формулам\n",
    "$$\n",
    "d_k=v_k-\\sum_{i=0}^{k-1}\\frac{d_i^TAv_k}{d_i^TAd_i}d_i\n",
    "$$\n",
    "* Выбрать произвольную точку $x_0$ и построить последовательность\n",
    "$$\n",
    "x_{k+1}=x_k-\\frac{d^T_k(Ax_k-b)}{d_k^TAd_k}d_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот алгоритм можно реализовать со сложностью $\\mathcal{O}(n^3)$ с памятью $\\mathcal{O}(n^2)$, для этого в формулах (1) необходимо запомнить векторы $d_i^TA$, тогда вычисление одного $d_k$ займет время порядка $\\mathcal{O}(k^2)$.\n",
    "\n",
    "<b>Определение</b>. <i>Подпространством Крылова</i> порядка $k$ матрицы $A$ и вектора $b$ называется\n",
    "$$\n",
    "\\mathcal{K}_k(A, b)=\\langle b, Ab, \\ldots, A^{k-1}b\\rangle, ~\\mathcal{K}_0(A, b)=\\{0\\}. \n",
    "$$\n",
    "Пусть $\\chi_A(t)=\\det(A-tI)=\\sum_{i=0}^{n-1}\\alpha_it^i-t^n$ -- характеристический полином $A$. Из теоремы Гамильтона-Кэли\n",
    "$$\n",
    "\\chi(A)=\\sum_{i=0}^{n-1}\\alpha_iA^i-A^n=0\n",
    "$$\n",
    "Умножая на $A^{-1}b$ и учитывая $\\alpha_0\\neq 0$ для положительно определенной матрицы $A$ получаем\n",
    "$$\n",
    "A^{-1}b=\\frac{1}{\\alpha_0}\\left(A^{n-1}b-\\sum_{i=1}^{n-1}\\alpha_iA^{i-1}b\\right)\\in\\mathcal{K}_n(A, b)\n",
    "$$\n",
    "<b>Определение</b>. <i>Последовательностью Крылова</i> функции $f(x)=\\frac{1}{2}x^TAx-b^Tx$ и начальной точкой $x_0$ называется последовательность\n",
    "$$\n",
    "x_k=argmin_{x\\in x_0+\\mathcal{K}_k(A, Ax_0-b)}f(x)\n",
    "$$\n",
    "Из условий оптимальности для $x_k$\n",
    "$$\n",
    "\\nabla f(x_k)=Ax_k-b \\bot \\mathcal{K}_k(A, Ax_0-b)\n",
    "$$\n",
    "С другой стороны, раз $x_k\\in x_0+\\mathcal{K}_k(A, Ax_0-b)$, то очевидным образом \n",
    "$Ax_k-b\\in Ax_0-b+A\\mathcal{K}_{k}(A,Ax_0-b)\\subset \\mathcal{K}_{k+1}(A, Ax_0-b)$.\n",
    "\n",
    "Таким образом $\\nabla f(x_k)\\notin \\mathcal{K}_k(A,Ax_0-b)$, но $\\nabla f(x_k)\\in \\mathcal{K}_{k+1}(A, Ax_0-b)$ $\\Rightarrow$\n",
    "$$\n",
    "\\mathcal{K}_k(A, Ax_0-b)=\\langle\\nabla f(x_0), \\ldots, \\nabla f(x_{k-1})\\rangle,\n",
    "$$\n",
    "в силу $\\nabla f(x_k)\\in\\mathcal{K}_k(A,Ax_0-b)^\\bot$ при $i\\neq j$\n",
    "$$\n",
    "\\nabla f(x_i)^T\\nabla f(x_j)=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея метода сопряженных градиентов: использование $v_k=\\nabla f(x_k)$ в методе сопряженных направлений. Использую индукцию покажем, что при $v_k=\\nabla f(x_k)$ метод сопряженных  направлений генерирует последоваельность Крылова для $f$, $x_0$: \n",
    "\n",
    "<b>База</b>. $k=0$ - тривиально.\n",
    "\n",
    "<b>Индукционный переход</b>. Пусть метод сгенерировал последовательность Крылова вплоть до $k-1$, тогда\n",
    "так как метод сопряженных направлений выбирает \n",
    "$$\n",
    "x_k=argmin_{x\\in x_0+\\langle d_0, \\ldots, d_{k-1}\\rangle}||x-x^*||_A^2,\n",
    "$$\n",
    "учитывая \n",
    "$$\n",
    "||x-x^*||_A^2=(x-x^*)^TA(x-x^*)=x^{*T}Ax^*-2x^{*T}Ax+x^TAx=||x^*||_A^2+2f(x)\n",
    "$$\n",
    "получаем, что минимизация $||x-x^*||_A$ равносильна минимизации $f(x)$. По построению\n",
    "$$\n",
    "\\langle d_0, \\ldots, d_{k-1}\\rangle=\\langle\\nabla f(x_0), \\ldots, \\nabla f(x_{k-1})\\rangle,\n",
    "$$ по\n",
    "индукционному предположению \n",
    "$$\n",
    "\\langle\\nabla f(x_0), \\ldots, \\nabla f(x_{k-1})\\rangle=\\mathcal{K}_k(A, Ax_0-b).~~\\#\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление $d_k$ сильно упрощается:\n",
    "$$\n",
    "v_i^T(Ax_{k+1}-b)=v_i^T(Ax_{k}-b)-\\alpha_{k}v_i^TAd_{k}\n",
    "$$\n",
    "$$\n",
    "v_i^TAd_{k}=\\frac{1}{\\alpha_{k}}\\left(v_i^Tv_{k}-v_i^Tv_{k+1}^T\\right)\n",
    "$$\n",
    "Так как $v_i^Tv_j=0$ при $i\\neq j$, то $d_i^TAv_k$ отлично от нуля только при $i=k$ или $i=k-1$. Итого шаг $2$ имеет вид\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "d_k&=v_k-\\sum_{i=0}^{k-1}\\frac{d_i^TAv_k}{d_i^TAd_i}d_i=v_k-\\frac{v_{k}^Tv_{k}}{\\alpha_{k-1}d_{k-1}^TAd_{k-1}}d_{k-1}\\\\\n",
    "&=v_k-\\frac{v_{k}^Tv_{k}}{d_{k-1}^Tv_{k-1}}d_{k-1}\n",
    "=v_k-\\frac{v_{k}^Tv_{k}}{v_{k-1}^Tv_{k-1}}d_{k-1}\n",
    "\\end{array}\n",
    "$$\n",
    "Это соотношение позволяет вычислять $d_k$ за время порядка $\\mathcal{O}(m)$, где $m$ - число ненулевых элементов $A$, что в итоге дает сложность $\\mathcal{O}(nm)$ с памятью $\\mathcal{O}(n+m)$. Асимптотическая скорость сходимости метода сопряженных градиентов совпадает с методом Чебышёва в силу того, что для последовательности Крылова действует схожий аргумент с многочленом:\n",
    "$$\n",
    "x\\in x_0+\\mathcal{K}_k(A, Ax_0-b)\\Leftrightarrow x= x_0+\\sum_{i=0}^{k-1}\\phi_iA^i(Ax_0-b)\n",
    "$$\n",
    "Если $Ax^*=b$, то\n",
    "$$\n",
    "x-x^*=x_0-x^*+\\sum_{i=0}^{k-1}\\phi_iA^iA(x_0-x^*)=\\left(I+\\sum_{i=1}^{k}\\phi_{i-1}A^i\\right)(x_0-x^*)\n",
    "$$\n",
    "Таким образом выбор $x_k$ как минимум $||x-x^*||_A$ на множестве $x_0+\\mathcal{K}_k(A, Ax_0-b)$ можно\n",
    "описать следующим образом\n",
    "$$\n",
    "x_k=x^*+\\|argmin_{P(0)=1,~\\deg P\\leq k}P(A)(x_0-x^*)\\|_A\n",
    "$$\n",
    "В частости из этого следует, что можно оценить скорость сходимости выбрав конкрет"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
