{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Введение в математическую и выпуклую оптимизацию\n",
    "Наиболее общая математическая трактовка задачи оптимизации выглядит следующим образом: дана функция $f$, заданная на множестве $\\mathcal{D}$, нужно найти точку $x^*\\in \\mathcal{D}$ такую, что\n",
    "$$\n",
    "f(x)\\geq f(x^*)~~\\forall x\\in\\mathcal{D}.\n",
    "$$\n",
    "Часто вышеописанное записывают как\n",
    "$$\n",
    "f(x)\\rightarrow min.\n",
    "$$\n",
    "Понятно, что задачи на поиск минимума легко сводятся к задачам на поиск максимума и наоборот. Исторически так сложилось, что в качестве базовой задачи используется именно задача на поиск минимума. К сожалению в таких общих условиях не существует какого-либо универсального метода для решения таких задач. Например если множество $\\mathcal{D}$ конечно и состоит из относительно небольшого числа точек, то их можно просто перебрать и найти минимум. Задача о разрешимости булевой функции также может быть трактована как задача поиска максимума функции, заданной на некотором конечном множестве. Однако, как известно, размер множества в задаче растет экспоненциально относительно количества переменных, что делает обычный перебор неэффективным, но все же возможным. Если же мы посмотрим на одномерную квадратичную функцию (заданную на $\\mathbb{R}$)\n",
    "<img border=2% style=\"float: right;\" width=25% height=25% src=\"images/par2.png\">\n",
    "$$\n",
    "f(x)=ax^2+bx+c,\n",
    "$$\n",
    "то перебор становится и вовсе невозможным, несмотря на то, что решение легко получается по формуле\n",
    "$$\n",
    "x^*=\\begin{cases}\n",
    "-\\frac{b}{2a} & a > 0 \\\\\n",
    "\\pm\\infty & a \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "В целом получается два больших класса задач для случая, когда $\\mathcal{D}$ дискретно, и когда $\\mathcal{D}$ непрерывно. Обычно задачи первого типа относят к <i>комбинаторной оптимизации</i>. Для задач с непрерывным $\\mathcal{D}$ разумно также ожидать, что и сама функция $f$ является непрерывной, иначе для любой точки, значение функции в которой мы не измеряли непосредственно, мы ничего не сможем гарантировать. Если же известно, что $\\mathcal{D}$ -- это единичный $n$-мерный куб, а для $f$ выполняется условие Липшица с константой $L$, т. е.\n",
    "$$\n",
    "f(x)-f(y)\\leq L\\|x-y\\|,\n",
    "$$\n",
    "то можно разбить исходный куб на кубики поменьше и посчитать значения функций в вершинах этим маленьких кубов. Если каждую сторону единичного куба мы разбили на $m$ частей (т.е. сторона маленького куба $1/m$), то мы знаем, что любая точка исходного куба находится на расстоянии не более\n",
    "<img style=\"float: right;\" width=20% height=20% src=\"images/cube.jpg\">\n",
    "$$\n",
    "\\frac{\\sqrt{n}}{2m}\n",
    "$$\n",
    "до ближайшей вершины маленького куба. Скомбинировав это с условием Липшица мы получаем, что\n",
    "$$\n",
    "\\min_{x\\in\\mathcal{D}_m}f(x)\\leq \\min_{x\\in\\mathcal{D}}f(x)+ \\frac{\\sqrt{n}L}{2m},\n",
    "$$\n",
    "где $\\mathcal{D}_m$ -- множество всех вершин маленьких квадратов. Таким образом мы не сможем найти точный минимум, но по крайней мере увеличивая $m$ возможно найти минимум с любой точностью. Это решение довольно громоздкое и по духу является переборным. Если же к непрерывности добавляется еще и дифференцируемость, то появляется возможным применение другой ключевой результат -- <i>теорема Ферма</i>: если $\\mathcal{D}$ -- открытое множество, а $f$ дифференцируема на $\\mathcal{D}$, то \n",
    "$$\n",
    "\\nabla f(x^*)=0\n",
    "$$\n",
    "Таким образом мы получили необходимые условия минимума и ... можем решить указанную систему уравнений и надеяться, что у нее конечное число решений, среди которых легко найти минимум. Этот результат был в последствие обобщён до случая, когда множество $\\mathcal{D}$ задачается в виде систем уравнений Лагранжем, и когда задается в виде систем уравнений и неравенств Карушем, Куном и Такером. Так или иначе это фундаментальные результаты, которые предоставляют <b>необходимые</b> условия. Здесь наконец мы подходим к тому, почему так важна категория задач выпуклой оптимизации. Дело в том, что если $f$ является выпуклой функцией, а $\\mathcal{D}$ является выпуклым множеством, то указанные выше условия являются <b>достаточными</b>. Это важный фактор, но не единственный.\n",
    "\n",
    "## Симплекс-метод как жадый алгоритм\n",
    "Линейное программирование и Симплекс-метод являются одним из самых значимых достижений математической оптимизации середины 20-го века. Задача линейного программирования обычно имеет следующий вид\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\mbox{минимизировать } & c^Tx,\\\\\n",
    "\\mbox{при условии } & Ax\\leq b, \\\\\n",
    "& x\\geq 0.\n",
    "\\end{array}\n",
    "$$\n",
    "Геометрически это задача заключается в том, чтобы найти на выпуклом многограннике самую дальнюю точку в направлении, заданном вектором $c$. Не сложно показать, что среди вершин многогранника всегда найдется точка минимума, а вершин ограниченное число, поэтому по меньшей мере их можно перебрать. Однако же выпуклость дает здесь еще одно важное свойство: если вершина не является точкой минимума, то среди соседних вершин найдется такая, что значение в ней меньше, чем в исходной. Это позволяет применить очень простой жадный алгоритм для поиска минимума: выбираем произвольную стартовую вершину и перемещаемся из нее в соседа с наименьшим значением функции, пока это дает уменьшение функции. Это и есть <b>симплекс-метод</b>, важным здесь является то, что именно выпуклость гарантирует глобальную оптимальность такой жадной стратегии.\n",
    "\n",
    "<img align=left width=45% height=45% src=\"images/Simplex-method-3-dimensions.png\">\n",
    "<img align=right width=50% height=50% src=\"images/animations/gif/simplex.gif\">\n",
    "\n",
    "## Полиномиальная разрешимость ЛП и выпуклая оптимизация\n",
    "Существуют [примеры](https://en.wikipedia.org/wiki/Klee–Minty_cube), когда симплекс-метод работает экспоненциально, тем не менее в болшистве случаев это очень эффективный и до сих пор актуальный алгоритм. Полиномиальная разрешимость задачи линейного программирования была доказано позже с развитием приближенных методов, таких как [метод внутренней точки](https://en.wikipedia.org/wiki/Interior-point_method) и [метод эллипсоидов](https://en.wikipedia.org/wiki/Ellipsoid_method)\n",
    "\n",
    "<img title=\"Метод внутренней точки\" align=left width=50% height=50% src=\"images/animations/gif/following_scheme.gif\">\n",
    "<img alt=\"Метод эллипсоидов\" align=right width=50% height=50% src=\"images/animations/gif/ellipse2.gif\">\n",
    "\n",
    "Здесь есть один небольшой подводный камень: для того, чтобы получить точное решение из приближенного используют соображение, что если приближенное решение достаточно близко к минимуму, то толучить из него точное решение достаточно просто. Близость к минимуму же тем не менее обусловлена размеру входных данных, из-за чего линейное программирование является слабо полиномиально разрешимой, что с точки зрения приближенных алгоритмов имеет следующий смысл: если $x\\in\\mathbb{R}^n,~A\\in\\mathbb{R}^{m\\times n}$, то задача линейного программирования может быть решена с точностью $\\epsilon$ за время, не превосходящая $P(\\log\\frac{1}{\\epsilon}, n, m)$, где $P$ -- полином. Возможность полиномиального решения задачи линейного программирования без привязки к $\\epsilon$ -- открытая задача, на практике же это не имеет особого значения.\n",
    "\n",
    "Примечательным в методах эллипсоидов и внутренней точки является то, что в отличие от симплекс метода, они не ограничены линейным программированием. Вот к примеру одна из стандартных формулировок задач, которую решает метод внутренней точки:\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\mbox{минимизировать } & f(x),\\\\\n",
    "\\mbox{при условии } & g_i(x)\\leq 0,~1\\leq i \\leq m, \\\\\n",
    "& Ax=b,\n",
    "\\end{array}\n",
    "$$\n",
    "где $f$, $g_i$ -- выпуклые функции.\n",
    "\n",
    "## Где применяется выпуклая оптимизация?\n",
    "Первым делом стоит отметить <b>теорию управления</b>, одной из основных задач которой является нахождение воздействия на систему, при котором система будет стабилизироваться. Такой системой может являться самолет в полете, корабль на плаву или скажем рабочий ядерный реактор. \n",
    "\n",
    "История выпуклой оптимизации в системах управления начинаются с работ Ляпунова, который показал, что динамическая система\n",
    "$$\n",
    "\\frac{d}{dt}x(t)=Ax(t)\n",
    "$$\n",
    "устойчива тогда и только тогда, когда найдется такая симетричная матрица $P$, что\n",
    "$$\n",
    "\\begin{cases}\n",
    "A^TP+PA\\prec 0, \\\\\n",
    "P\\succ 0,\n",
    "\\end{cases}\n",
    "$$\n",
    "где $A\\succ B$ означает, что $A-B$ -- положительно определенная матрица. Современная теория оптимизации относит такие задачи к виду <b>линейных матричных неравенств</b>. Во времена Ляпунова (и долгое время после него) подобные системы решались руками, с развитием выпуклой оптимизации возникла возможность подбирать управление алгоритмически причем для гораздо более широкого класса задач.\n",
    "\n",
    "Другим важным примером является <b>машинное обучение</b>. Несмотря на то, машинном обучении выпуклыми задачами является только малая часть, в подавляющем большинстве случаев используется методы выпуклой оптимизации, показывающие хорошие результаты несмотря на то, что они не гарантируют нахождение глобального минимума в этих задачах.\n",
    "<img style=\"float: right;\" width=35% height=35% src=\"images/polynomial_fit.png\">\n",
    "\n",
    "В целом задача машинного обучения может быть сформулирована следующим образом. Есть некоторое параметрическое семейство функций $f(\\cdot, \\theta)$ и набор данных ${x_i,y_i}_{i=1}^m$, мы хотим подобрать параметры $\\theta$ так, чтобы $f(\\cdot, \\theta)$ как можно лучше соответствовала этим данным. Этот подбор можно сформулировать в виде задачи оптимизации если ввести дополнительно функцию штрафа несоответствия $g$ и минимизировать\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(\\theta)=\\frac{1}{m}\\sum_{i=1}^mg(f(x_i, \\theta), y_i)\\rightarrow \\min\n",
    "$$\n",
    "\n",
    "Применение найденных параметров $\\theta$ всегда одно и то же: если $f(\\cdot, \\theta)$ хорошо аппроксимирует известные данные, то её можно использовать для предсказания $y$ по ранее неизвестным $x$. Здесь мы вольны выбирать функцию $g$ на свое усмотрение, однако в целом нам интересны функции, которые тем меньше, чем меньше разница между её аргументами, например подойдет любая метрика. Например если мы возьмем\n",
    "\n",
    "<img style=\"float: right;\" width=35% height=35% src=\"images/logistic_regression.png\">\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "f(x, \\theta)=\\sum_{i=1}^nx^i\\theta_i~~(x\\in \\mathbb{R})\\\\\n",
    "g(x, y)=(x-y)^2,\n",
    "\\end{cases}\n",
    "$$\n",
    "то получим задачу полиномиальной регрессии, которая сводится к системе линейных уравнений. Также мы можем получить выпуклую задачу в случае логистической регрессии\n",
    "$$\n",
    "\\begin{cases}\n",
    "f(x, \\theta)=\\frac{1}{1+xp(\\theta^Tx)}\\\\\n",
    "g(x, y)=-y\\log x-(1-y)\\log(1-x),\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "К сожалению используемые на практике модели машинного обучения приводят к невыпуклым задачам, тем не менее оказывается, что методы выпуклой оптимизации все еще могут использоваться для их обучения. Например для нейросетевых классификаторов стандартными методами обучения являются разновидности стохастического градиентного спуска.\n",
    "\n",
    "Применений выпуклой оптимизации очень много, но хотелось бы выделить еще одно -- <b>оригами</b>. Не так давно в 90х годах в оригами произошел серьезный прорыв -- появились способы компьютерного моделирование паттернов, основанного на задач упаковки кругов на плоскости. Несмотря на то, что эта задача не является выпуклой, оригинальный метод основывался на использовании инструментов выпуклой оптимизации. Сама задача помогала вычислить паттерн изгибов из \"спичечных\" фигурок.\n",
    "\n",
    "\n",
    "<img align=center width=70% height=70% src=\"images/computational_origami.png\">\n",
    "\n",
    "<img align=center width=70% height=70% src=\"images/deer.jpg\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}